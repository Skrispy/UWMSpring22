{\rtf1\ansi\ansicpg1252\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 TimesNewRomanPS-BoldMT;\f1\froman\fcharset0 Times-Roman;\f2\froman\fcharset0 TimesNewRomanPS-ItalicMT;
\f3\fswiss\fcharset0 Arial-ItalicMT;\f4\froman\fcharset0 TimesNewRomanPSMT;\f5\froman\fcharset0 TimesNewRomanPS-BoldItalicMT;
\f6\fswiss\fcharset0 ArialMT;}
{\colortbl;\red255\green255\blue255;\red251\green0\blue7;\red0\green0\blue0;\red29\green29\blue29;
\red255\green255\blue255;\red49\green49\blue49;}
{\*\expandedcolortbl;;\cssrgb\c100000\c0\c0;\cssrgb\c0\c0\c0;\cssrgb\c14902\c14902\c14902;
\cssrgb\c100000\c100000\c100000;\cssrgb\c25098\c25098\c25098;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid5}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa133\qc\partightenfactor0

\f0\b\fs36 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Intelligent User Interfaces
\f1\b0\fs24 \cf0 \strokec3 \

\f0\b\fs36 Research Results Paper 11 Report
\f1\b0\fs24 \
\pard\pardeftab720\sa133\partightenfactor0

\f0\b\fs28 \cf0 \ul \ulc0 Reviewer:
\f1\b0\fs24 \ulnone \

\f2\i Samuel Kersebet
\f1\i0 \

\f0\b\fs28 \ul Topic Area:
\f1\b0\fs24 \ulnone \

\f2\i Motor Impairments, ability-based user interfaces, Supple, Supple++ Ann Arnauld
\f1\i0 \

\f0\b\fs28 \ul Bibliographic Information:
\f1\b0\fs24 \ulnone \
\pard\pardeftab720\partightenfactor0

\f3\i \cf4 \cb5 \strokec4 Krzysztof Z. Gajos, Jacob O. Wobbrock, and Daniel S. Weld. 2008. Improving the performance of motor-impaired users with automatically-generated, ability-based interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '08). Association for Computing Machinery, New York, NY, USA, 1257\'961266. DOI:https://doi.org/10.1145/1357054.1357250
\f1\i0 \cf0 \strokec3 \
\cb1 \
\pard\pardeftab720\sa133\partightenfactor0

\f0\b\fs28 \cf0 \ul \ulc0 Summary:
\f1\b0\fs24 \ulnone \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\li720\fi-720\partightenfactor0

\f4\fs32 \cf0 The primary problem that the author hopes to address is to help physically disabled individuals to interact more effectively with their computers.\'a0
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\li720\fi-720\partightenfactor0

\f4\fs32 \cf0 The authors' main contribution to the solution of this problem is developing automatic generation tools used to create the user interfaces and then conducting an experiment to explore efficiency gains through different optimizations.\'a0
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\li720\fi-720\partightenfactor0

\f4\fs32 \cf0 Another interesting contribution of the work is learning that even able-bodied individuals benefited from less error and better time efficiency when using an ability based interface. Giving people snap shots of interface choices, also testing not on the decision making and following the directions of the orange boxes.\'a0
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\li720\fi-720\partightenfactor0

\f4\fs32 \cf0 The most important limitation of their solution is that the applications are relatively simple and minimal in their design, not to mention rather trivial. Did not include any web based applications which are very prominent in use today. They did not inquire about the apps that the users interacted with most, they should have learned more about their target audience.\'a0
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\li720\fi-720\partightenfactor0

\f4\fs32 \cf0 According to the authors, the previous work most closely related to theirs is \'93Mankoff, J., Dey, A., Batra, U., and Moore, M. Web accessibility for low bandwidth input. Proc. Assets \'9202. ACM Press, New York, NY, USA, 2002, 17\'9624.\'94 but that work didn't solve the problem entirely because the tool only works for web pages and does not adapt to the users specific disabilities.\'a0
\f1\fs24 \

\f4\fs32 	
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\li720\fi-720\partightenfactor0

\f4\fs32 \cf0 This system was evaluated by an experiment involving 11 participants with varying motor impairments and 6 able-bodied participants recruited from the Puget Sound area. The participants were asked to elicit their preferences in a GUI as well as taking an ability test to explore their motor capabilities. The tests were used to generate two unique GUI variants for each application(print, font and synthetic) and metrics were recorded to compare how users performed among the three variants. Measures taken included, Widget manipulation time, Interface navigation time, total time, and error rate. There was also a subjective analysis taken to explore how the users found the experience with each variant.\'a0
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\li720\fi-720\partightenfactor0

\f4\fs32 \cf0 One other possible way to evaluate the work would be to do blind tests of each variant of each application, there may be some level of training when a user interacts with an application multiple times so it may be more accurate to have each participant only experience one GUI. Talking aloud while the user engages with the system could give the researchers more information that allows for more immediate feedback.\'a0
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\
\pard\pardeftab720\li720\fi-720\partightenfactor0

\f4\fs32 \cf0 Since publication, the impact of this paper has been medium, having (110 citations\'a0 ACM) (228 google) (153 from research gate) 1 citation 25 days ago, 9 from 2021. This Paper won a best paper award. This paper itself may not have had a large impact but the project as a whole has been cited many times and is well known in the HCI community.\'a0
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\li720\fi-720\partightenfactor0

\f4\fs32 \cf0 There are resources related to this paper online at https://www.youtube.com/watch?v=B63whNtp4qc
\f1\fs24 \
\pard\pardeftab720\li1413\fi-1414\partightenfactor0

\f4\fs32 \cf0 Link to SUPPLE Demonstration.\'a0
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\li1413\fi-1414\partightenfactor0

\f4\fs32 \cf0 This paper has been cited 158? times, including\'a0 these two other papers:
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0

\f2\i\fs32 \cf4 \cb5 \strokec4 Brittany Lewis and Krishna Venkatasubramanian. 2021. 
\f5\b \ul \'93I...Got my Nose-Print. But it Wasn\'92t Accurate\'94: How People with Upper Extremity Impairment Authenticate on their Personal Computing Devices.
\f2\b0 \ulnone  In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21). Association for Computing Machinery, New York, NY, USA, Article 379, 1\'9614. DOI:https://doi.org/10.1145/3411764.3445070
\f1\i0\fs24 \cf0 \strokec3 \
\'a0\

\f4\fs32 Wobbrock J. \ul Improving Pointing in Graphical User Interfaces for People with Motor Impairments Through Ability-Based Design\ulnone  Assistive Technologies and Computer Access for Motor Disabilities. 10.4018/978-1-4666-4438-0.ch008. (206-253)
\f1\fs24 \cb1 \
\
\
\
\pard\pardeftab720\sa133\partightenfactor0

\f0\b\fs32 \cf0 One interesting thing that the authors are doing now is
\f1\b0\fs24 \

\f4\fs32 \cb5 Krzysztof Z. Gajos is a professor at Harvard leading the Intelligent Interactive systems group there. This group focuses on how to build AI powered systems that are well suited for the strengths and limitations of human cognition perception and behavior. He also runs a fun project called lab in the wild http://labinthewild.org/index.php
\f1\fs24 \cb1 \
\pard\pardeftab720\partightenfactor0

\f4\fs32 \cf4 \cb5 \strokec4 Jacob O. Wobbrock is a professor at University of Washington. He is also a co-founder of a company called AnswerDash(AI customer support) that sold in 2020. He is now the Co-Director at UW for the \cf0 \strokec3 Center for Research and Education on Accessible Technology & Experiences (CREATE)
\f1\fs24 \
\'a0\
\pard\pardeftab720\partightenfactor0

\f4\fs32 \cf0 Daniel S. Weld is a professor at UW as well leaning more in the direction of AI, having many papers and two books published, not to mention being the founding editor of the journal of AI Research. He is also a serial entrepreneur, as well as being a venture partner of multiple venture groups.\'a0
\f1\fs24 \
\'a0\
\'a0\
\pard\pardeftab720\partightenfactor0
\cf0 \cb1 \
\pard\pardeftab720\sa133\partightenfactor0

\f0\b\fs28 \cf0 \ul \ulc0 Selected Class questions:
\f1\b0\fs24 \ulnone \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0

\f4 \cf0 \'a0
\f1 \

\f6\fs32 \cf6 \cb5 \strokec6 Q. Is UI(User Interfaces) generated by SUPPLE++ measured only by user's motor capabilities? ( I mean that can we add voice inputs to generate UI instead of motor capabilities?) - Shubham Shivajirao Pawar
\f1\fs24 \cf0 \cb1 \strokec3 \
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f6\fs32 \cf6 \cb5 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 The role of SUPPLE++ is to assess the users input abilities through a series of tests. It seems if an interface for voice were implemented that a suite of tests could also be added to include commands being input to the machine via that medium. It would take some adaptation but this seems like a quite feasible adaptation for this framework.\cb1 \
\pard\pardeftab720\partightenfactor0

\f1\fs24 \cf0 \strokec3 \
\pard\pardeftab720\partightenfactor0

\f6\fs32 \cf6 \cb5 \strokec6 Q. The author concludes that the people with motor impairments were faster when compared to the able-bodied ones. What is the main reason as to why the able-bodied were not faster or which part of the system did they actually not understand? - Sudishna Karne
\f1\fs24 \cf0 \cb1 \strokec3 \
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0
\f6\fs32 \cf6 \cb5 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 The results do not actually conclude that the motor-impared subjects performed better than the able-bodied subjects, but there was however a significant improvement in efficiency for motor-impared subjects navigating the UI (on average 26.4%). This helped close the gap between able bodied and motor-impared by an average of 62%.\'a0\cb1 \
\pard\pardeftab720\partightenfactor0

\f1\fs24 \cf0 \strokec3 \
\pard\pardeftab720\partightenfactor0

\f6\fs32 \cf6 \cb5 \strokec6 Q. Were the initial results of personalization preference at odds with the reported preference and performance of ability elicitation vs preference elicitation? - Nathan Leverence
\f1\fs24 \cf0 \cb1 \strokec3 \
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\f6\fs32 \cf6 \cb5 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 The initial results with the preference based elicitation and the ability based eilicitation were at odds in the study. Both participant groups showed the majority of users enjoying the preference based testing(based on a Likert scale evaluation).\'a0 But when the models were actually produced and used, the emprical results showed that the ability elicitation model generated by SUPPLE++ performed best.\cb1 \
\pard\pardeftab720\partightenfactor0

\f1\fs24 \cf0 \strokec3 \
\pard\pardeftab720\partightenfactor0

\f6\fs32 \cf6 \cb5 \strokec6 Q. In the diagram labeled figure 2. It shows a diagram of the systems used in the studies. It doesn't show any sort of feedback loop. Does the supple+++ continuously model the users abilities as they use the programs or does it take a measurement at the start and create a model that it users from then on? If a user had a condition that became better or worse over time would supple+++ be able to adapt to the user's change in needs? - Kevin Michael Kratzke
\f1\fs24 \cf0 \cb1 \strokec3 \
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0
\f6\fs32 \cf6 \cb5 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 The model does not currently support a feedback loop for adapting to a user with evolving abilities. It would require a user to re test their skills to generate the optimal UI. This feedback loop feature is a good idea, however changes should not be made too freaquently or the user may be confused by the shifting UIs.\'a0\cb1 \
\pard\pardeftab720\partightenfactor0

\f1\fs24 \cf0 \strokec3 \
\pard\pardeftab720\partightenfactor0

\f6\fs32 \cf6 \cb5 \strokec6 Q. The paper mentions that in the Ability Elicitation Dragging Task, a set of reciprocal dragging tasks were used. Could explain the significance of using reciprocal dragging task in the author\'92s approach ? - Shyanka Basak
\f1\fs24 \cf0 \cb1 \strokec3 \
\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0
\f6\fs32 \cf6 \cb5 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 The ability elicitation dragging task and its recipricol tasks simply refers to the set of tasks employed in the SUPPLE++ testing. The tests involved clicking and draging a list selection, dragging a point to a line, clicking a target and pointing at various areas on the screen in succession. The results from the error and time to complete these tasks was then calculated to evaluate what type of interface would be optimal for the user.\'a0\cb1 \
\pard\pardeftab720\partightenfactor0

\f1\fs24 \cf0 \strokec3 \
\pard\pardeftab720\partightenfactor0

\f6\fs32 \cf6 \cb5 \strokec6 Q. In the paper, error rate is based on the clicks that don't set the correct values in the target widget. Do you think the system considers the missed-clicks (clicks made other than the target like the gaps between checkboxes and buttons)? - Sharath Chander Pugazhenthi
\f1\fs24 \cf0 \cb1 \strokec3 \
\

\f6\fs32 \cf6 \cb5 \strokec6 A. In the evaluation of the generated model, there was a very clear visual aid that showed the user what task they needed to accomplish and where they needed to do the task on the screen. It is to my understanding that any action outside of this defined task area was accounted for in the error rate.\'a0
\f1\fs24 \cf0 \cb1 \strokec3 \
}